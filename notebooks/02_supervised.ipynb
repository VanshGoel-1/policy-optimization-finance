{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd0019b8-e811-4846-8b12-6aac0d885c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f067ead-bc3e-4618-a9af-a394f057d58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n",
      "Training data loaded: X shape (312934, 12), y shape (312934,)\n",
      "Test data loaded:     X shape (78234, 12), y shape (78234,)\n",
      "\n",
      "First 5 rows of scaled training features:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loan_amnt</th>\n",
       "      <th>term</th>\n",
       "      <th>int_rate</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>dti</th>\n",
       "      <th>emp_length</th>\n",
       "      <th>pub_rec</th>\n",
       "      <th>revol_util</th>\n",
       "      <th>total_acc</th>\n",
       "      <th>open_acc</th>\n",
       "      <th>mort_acc</th>\n",
       "      <th>fico_range_low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41094</th>\n",
       "      <td>0.737034</td>\n",
       "      <td>1.744741</td>\n",
       "      <td>0.040160</td>\n",
       "      <td>0.313690</td>\n",
       "      <td>-0.970261</td>\n",
       "      <td>1.117687</td>\n",
       "      <td>-0.365734</td>\n",
       "      <td>-1.578159</td>\n",
       "      <td>0.791257</td>\n",
       "      <td>0.736024</td>\n",
       "      <td>0.169932</td>\n",
       "      <td>0.028159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91912</th>\n",
       "      <td>-1.078352</td>\n",
       "      <td>-0.573151</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>-0.508550</td>\n",
       "      <td>1.983979</td>\n",
       "      <td>1.117687</td>\n",
       "      <td>-0.365734</td>\n",
       "      <td>-0.549676</td>\n",
       "      <td>-0.112613</td>\n",
       "      <td>0.558369</td>\n",
       "      <td>-0.332658</td>\n",
       "      <td>0.676154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201663</th>\n",
       "      <td>0.684667</td>\n",
       "      <td>1.744741</td>\n",
       "      <td>1.335930</td>\n",
       "      <td>-0.182306</td>\n",
       "      <td>1.805061</td>\n",
       "      <td>1.117687</td>\n",
       "      <td>-0.365734</td>\n",
       "      <td>0.495395</td>\n",
       "      <td>0.051727</td>\n",
       "      <td>-0.152254</td>\n",
       "      <td>-0.835247</td>\n",
       "      <td>-0.295838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491903</th>\n",
       "      <td>-0.025195</td>\n",
       "      <td>-0.573151</td>\n",
       "      <td>1.292433</td>\n",
       "      <td>-0.150478</td>\n",
       "      <td>1.575626</td>\n",
       "      <td>-1.660889</td>\n",
       "      <td>-0.365734</td>\n",
       "      <td>-0.375498</td>\n",
       "      <td>-0.030443</td>\n",
       "      <td>0.203057</td>\n",
       "      <td>0.672522</td>\n",
       "      <td>0.676154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154509</th>\n",
       "      <td>1.784372</td>\n",
       "      <td>1.744741</td>\n",
       "      <td>1.734276</td>\n",
       "      <td>0.101499</td>\n",
       "      <td>0.048515</td>\n",
       "      <td>-0.271601</td>\n",
       "      <td>-0.365734</td>\n",
       "      <td>-0.354762</td>\n",
       "      <td>0.380407</td>\n",
       "      <td>1.091335</td>\n",
       "      <td>-0.835247</td>\n",
       "      <td>-0.619836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loan_amnt      term  int_rate  annual_inc       dti  emp_length  \\\n",
       "41094    0.737034  1.744741  0.040160    0.313690 -0.970261    1.117687   \n",
       "91912   -1.078352 -0.573151 -0.028520   -0.508550  1.983979    1.117687   \n",
       "201663   0.684667  1.744741  1.335930   -0.182306  1.805061    1.117687   \n",
       "491903  -0.025195 -0.573151  1.292433   -0.150478  1.575626   -1.660889   \n",
       "154509   1.784372  1.744741  1.734276    0.101499  0.048515   -0.271601   \n",
       "\n",
       "         pub_rec  revol_util  total_acc  open_acc  mort_acc  fico_range_low  \n",
       "41094  -0.365734   -1.578159   0.791257  0.736024  0.169932        0.028159  \n",
       "91912  -0.365734   -0.549676  -0.112613  0.558369 -0.332658        0.676154  \n",
       "201663 -0.365734    0.495395   0.051727 -0.152254 -0.835247       -0.295838  \n",
       "491903 -0.365734   -0.375498  -0.030443  0.203057  0.672522        0.676154  \n",
       "154509 -0.365734   -0.354762   0.380407  1.091335 -0.835247       -0.619836  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Task 2, Cell 1: Imports & Load Processed Data ===\n",
    "# We are in notebooks/02_supervised.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch - our deep learning library\n",
    "import torch\n",
    "import torch.nn as nn  # nn = Neural Network\n",
    "import torch.optim as optim # optim = Optimizers (like Adam)\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Scikit-learn - for metrics and utilities\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report\n",
    "\n",
    "print(\"All libraries imported successfully.\")\n",
    "\n",
    "# --- Load the Processed Data ---\n",
    "# We load the .parquet files we created in Task 1.\n",
    "train_path = '../data/processed/train.parquet'\n",
    "test_path = '../data/processed/test.parquet'\n",
    "\n",
    "train_data = pd.read_parquet(train_path)\n",
    "test_data = pd.read_parquet(test_path)\n",
    "\n",
    "# 1. Separate features (X) and target (y)\n",
    "X_train = train_data.drop(columns=['target'])\n",
    "y_train = train_data['target']\n",
    "\n",
    "X_test = test_data.drop(columns=['target'])\n",
    "y_test = test_data['target']\n",
    "\n",
    "print(f\"Training data loaded: X shape {X_train.shape}, y shape {y_train.shape}\")\n",
    "print(f\"Test data loaded:     X shape {X_test.shape}, y shape {y_test.shape}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of scaled training features:\")\n",
    "display(X_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332784ae-cc46-4aa5-beca-e5c1b7b3d578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Data Converted to Tensors ---\n",
      "X_train_tensor shape: torch.Size([312934, 12]), dtype: torch.float32\n",
      "y_train_tensor shape: torch.Size([312934]), dtype: torch.float32\n",
      "\n",
      "--- Datasets Created ---\n",
      "Length of train_dataset: 312934\n",
      "Length of test_dataset:  78234\n",
      "\n",
      "--- DataLoaders Created ---\n",
      "Number of batches in train_loader: 306\n",
      "Number of batches in test_loader:  77\n"
     ]
    }
   ],
   "source": [
    "# === Task 2, Cell 2: Create a PyTorch Dataset ===\n",
    "\n",
    "# First, we need to convert our pandas DataFrames into torch Tensors.\n",
    "# A Tensor is the main data type used by PyTorch (like a numpy array).\n",
    "\n",
    "# We want our data to be 'float32' (decimal numbers)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "print(\"--- Data Converted to Tensors ---\")\n",
    "print(f\"X_train_tensor shape: {X_train_tensor.shape}, dtype: {X_train_tensor.dtype}\")\n",
    "print(f\"y_train_tensor shape: {y_train_tensor.shape}, dtype: {y_train_tensor.dtype}\")\n",
    "\n",
    "\n",
    "# --- Create a custom Dataset class ---\n",
    "# This is a standard PyTorch pattern.\n",
    "# It tells PyTorch how to get one item ( __getitem__ ) and\n",
    "# how many items there are ( __len__ ).\n",
    "\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # This just returns the total number of rows\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This returns one row of data at a time\n",
    "        # 'idx' is the row number (e.g., 5)\n",
    "        x = self.features[idx]\n",
    "        y = self.targets[idx]\n",
    "        # We need to add an extra dimension to y for the loss function\n",
    "        return x, y.unsqueeze(0)\n",
    "\n",
    "# Now, create instances of our Dataset\n",
    "train_dataset = LoanDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = LoanDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "print(\"\\n--- Datasets Created ---\")\n",
    "print(f\"Length of train_dataset: {len(train_dataset)}\")\n",
    "print(f\"Length of test_dataset:  {len(test_dataset)}\")\n",
    "\n",
    "# --- Create DataLoaders ---\n",
    "# DataLoaders automatically handle batching, shuffling, etc.\n",
    "# BATCH_SIZE = 1024 is a good starting point.\n",
    "# It means we'll feed the model 1024 loans at a time.\n",
    "BATCH_SIZE = 1024\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True) # Shuffle training data each epoch\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False) # No need to shuffle test data\n",
    "\n",
    "print(f\"\\n--- DataLoaders Created ---\")\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "print(f\"Number of batches in test_loader:  {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f417f45-45cc-4dcf-83d8-ae4b8526d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Architecture Created ---\n",
      "MLP(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=12, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.3, inplace=False)\n",
      "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n",
      "\n",
      "--- Loss Function and Optimizer Defined ---\n",
      "Loss Function: BCELoss()\n",
      "Optimizer: Adam\n"
     ]
    }
   ],
   "source": [
    "# === Task 2, Cell 3: Define the Neural Network ===\n",
    "\n",
    "# How many features are going in? (e.g., 12)\n",
    "INPUT_FEATURES = X_train.shape[1] \n",
    "# How many predictions are coming out? (e.g., 1)\n",
    "OUTPUT_FEATURES = 1\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    # This is the \"blueprint\" for our network\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        # We define our network as a sequence of layers\n",
    "        self.layers = nn.Sequential(\n",
    "            # 1. First hidden layer: takes 12 features in, outputs 64\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),  # \"Rectified Linear Unit\" activation\n",
    "            nn.Dropout(0.3), # Drops 30% of neurons to prevent overfitting\n",
    "            \n",
    "            # 2. Second hidden layer: takes 64 in, outputs 32\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            # 3. Output layer: takes 32 in, outputs 1\n",
    "            nn.Linear(32, output_dim),\n",
    "            nn.Sigmoid() # Squashes the output to be between 0 and 1\n",
    "        )\n",
    "\n",
    "    # This function defines how data \"flows\" through the layers\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# --- Create an instance of our model ---\n",
    "model = MLP(INPUT_FEATURES, OUTPUT_FEATURES)\n",
    "\n",
    "# Print the model's architecture\n",
    "print(\"--- Model Architecture Created ---\")\n",
    "print(model)\n",
    "\n",
    "# --- Define Loss Function and Optimizer ---\n",
    "\n",
    "# Loss Function: How we measure \"how wrong\" the model is.\n",
    "# \"Binary Cross Entropy\" is the standard for 0/1 classification.\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizer: The \"engine\" that updates the model's weights.\n",
    "# \"Adam\" is a smart, popular choice.\n",
    "# lr=0.001 is the \"learning rate\" - how big of a step to take.\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"\\n--- Loss Function and Optimizer Defined ---\")\n",
    "print(f\"Loss Function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "832fb37f-a252-4ea9-b755-a663583276e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 2, Cell 4: Define the Training Function ===\n",
    "# This function will run one full pass over the training data\n",
    "\n",
    "def train_epoch(model, data_loader, criterion, optimizer):\n",
    "    \n",
    "    # 1. Set the model to \"training mode\"\n",
    "    #    This tells layers like Dropout that they should be active.\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    # 2. Loop over every batch of data in the data_loader\n",
    "    for features, targets in data_loader:\n",
    "        \n",
    "        # 3. Clear old gradients\n",
    "        #    PyTorch accumulates gradients, so we reset them each time.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 4. Forward Pass: Make a prediction\n",
    "        #    This \"pushes\" the 'features' through the model.\n",
    "        predictions = model(features)\n",
    "        \n",
    "        # 5. Calculate Loss: How wrong were we?\n",
    "        #    Compare the model's 'predictions' to the true 'targets'.\n",
    "        loss = criterion(predictions, targets)\n",
    "        \n",
    "        # 6. Backward Pass: Calculate gradients\n",
    "        #    This is the \"learning\" step. It calculates how much\n",
    "        #    each tiny weight in the model contributed to the error.\n",
    "        loss.backward()\n",
    "        \n",
    "        # 7. Update Weights: Tell the optimizer to take a step\n",
    "        #    The optimizer uses the gradients to update the weights.\n",
    "        optimizer.step()\n",
    "        \n",
    "        # --- Store results for metrics ---\n",
    "        total_loss += loss.item() # .item() gets the raw number\n",
    "        \n",
    "        # 'predictions' are probabilities (e.g., 0.7).\n",
    "        # We round them (0.7 -> 1.0) to get a final 0/1 prediction.\n",
    "        preds_binary = torch.round(predictions)\n",
    "        \n",
    "        # .detach().numpy() moves data from PyTorch back to NumPy\n",
    "        all_preds.extend(preds_binary.detach().numpy())\n",
    "        all_targets.extend(targets.detach().numpy())\n",
    "\n",
    "    # --- Calculate metrics for the whole epoch ---\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    accuracy = accuracy_score(all_targets, all_preds)\n",
    "    f1 = f1_score(all_targets, all_preds)\n",
    "    \n",
    "    return avg_loss, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8cb701-b72d-4e03-a896-26ac6f70e601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Task 2, Cell 5: Define the Evaluation Function ===\n",
    "# This function will run one full pass over the *test* data\n",
    "\n",
    "def evaluate_epoch(model, data_loader, criterion):\n",
    "    \n",
    "    # 1. Set the model to \"evaluation mode\"\n",
    "    #    This tells layers like Dropout to turn OFF.\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    all_targets = []\n",
    "    all_preds_probs = [] # We'll store the raw probabilities for AUC\n",
    "\n",
    "    # 2. Tell PyTorch not to calculate gradients\n",
    "    #    This saves memory and computation.\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # 3. Loop over every batch of data\n",
    "        for features, targets in data_loader:\n",
    "            \n",
    "            # 4. Forward Pass: Make a prediction\n",
    "            predictions = model(features)\n",
    "            \n",
    "            # 5. Calculate Loss\n",
    "            loss = criterion(predictions, targets)\n",
    "            \n",
    "            # --- Store results for metrics ---\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Store the raw probabilities (e.g., 0.7) for AUC\n",
    "            all_preds_probs.extend(predictions.numpy())\n",
    "            # Store the true targets\n",
    "            all_targets.extend(targets.numpy())\n",
    "\n",
    "    # --- Calculate metrics for the whole epoch ---\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    # Convert probabilities to binary 0/1 predictions\n",
    "    all_preds_binary = np.round(all_preds_probs)\n",
    "    \n",
    "    # Calculate all our metrics\n",
    "    accuracy = accuracy_score(all_targets, all_preds_binary)\n",
    "    f1 = f1_score(all_targets, all_preds_binary)\n",
    "    \n",
    "    # AUC is the most important metric!\n",
    "    # It uses the raw probabilities, not the rounded 0/1 predictions.\n",
    "    try:\n",
    "        auc = roc_auc_score(all_targets, all_preds_probs)\n",
    "    except ValueError:\n",
    "        auc = 0.5 # Handle edge cases\n",
    "    \n",
    "    return avg_loss, accuracy, f1, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24f39b79-5b53-4015-8281-defc008ad71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Model Training ---\n",
      "\n",
      "Epoch: 01/5\n",
      "\tTime: 0.14 minutes\n",
      "\tTrain Loss: 0.4740 | Train Acc: 79.87% | Train F1: 0.1006\n",
      "\tVal. Loss:  0.4471 | Val. Acc:  80.24% | Val. F1:  0.1658\n",
      "\t*** Val. AUC: 0.7297 ***\n",
      "\n",
      "Epoch: 02/5\n",
      "\tTime: 0.15 minutes\n",
      "\tTrain Loss: 0.4544 | Train Acc: 80.22% | Train F1: 0.1685\n",
      "\tVal. Loss:  0.4458 | Val. Acc:  80.37% | Val. F1:  0.1773\n",
      "\t*** Val. AUC: 0.7317 ***\n",
      "\n",
      "Epoch: 03/5\n",
      "\tTime: 0.14 minutes\n",
      "\tTrain Loss: 0.4526 | Train Acc: 80.24% | Train F1: 0.1744\n",
      "\tVal. Loss:  0.4456 | Val. Acc:  80.37% | Val. F1:  0.1935\n",
      "\t*** Val. AUC: 0.7321 ***\n",
      "\n",
      "Epoch: 04/5\n",
      "\tTime: 0.14 minutes\n",
      "\tTrain Loss: 0.4520 | Train Acc: 80.23% | Train F1: 0.1776\n",
      "\tVal. Loss:  0.4456 | Val. Acc:  80.37% | Val. F1:  0.1825\n",
      "\t*** Val. AUC: 0.7327 ***\n",
      "\n",
      "Epoch: 05/5\n",
      "\tTime: 0.13 minutes\n",
      "\tTrain Loss: 0.4513 | Train Acc: 80.25% | Train F1: 0.1779\n",
      "\tVal. Loss:  0.4452 | Val. Acc:  80.33% | Val. F1:  0.2348\n",
      "\t*** Val. AUC: 0.7329 ***\n",
      "\n",
      "--- Training Complete ---\n"
     ]
    }
   ],
   "source": [
    "# === Task 2, Cell 6: Run the Training Loop ===\n",
    "import time\n",
    "\n",
    "NUM_EPOCHS = 5 # An epoch is one full pass over the training data\n",
    "\n",
    "print(\"--- Starting Model Training ---\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Train for one epoch\n",
    "    train_loss, train_acc, train_f1 = train_epoch(\n",
    "        model, train_loader, criterion, optimizer\n",
    "    )\n",
    "    \n",
    "    # 2. Evaluate on the test set\n",
    "    val_loss, val_acc, val_f1, val_auc = evaluate_epoch(\n",
    "        model, test_loader, criterion\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    epoch_mins = (end_time - start_time) / 60\n",
    "    \n",
    "    # 3. Print the results for this epoch\n",
    "    print(f\"\\nEpoch: {epoch:02}/{NUM_EPOCHS}\")\n",
    "    print(f\"\\tTime: {epoch_mins:.2f} minutes\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}% | Train F1: {train_f1:.4f}\")\n",
    "    print(f\"\\tVal. Loss:  {val_loss:.4f} | Val. Acc:  {val_acc*100:.2f}% | Val. F1:  {val_f1:.4f}\")\n",
    "    print(f\"\\t*** Val. AUC: {val_auc:.4f} ***\")\n",
    "\n",
    "print(\"\\n--- Training Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dd1cd81-8d63-4642-a214-52ee2a48bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running Final Evaluation on Test Set ---\n",
      "\n",
      "--- Deep Learning Model Metrics (Test Set) ---\n",
      "     Accuracy: 80.33%\n",
      "     F1-Score: 0.2348\n",
      "ROC AUC Score: 0.7329\n",
      "\n",
      "--- Classification Report ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "   Class 0 (Paid)       0.82      0.97      0.89     62468\n",
      "Class 1 (Default)       0.54      0.15      0.23     15766\n",
      "\n",
      "         accuracy                           0.80     78234\n",
      "        macro avg       0.68      0.56      0.56     78234\n",
      "     weighted avg       0.76      0.80      0.76     78234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Task 2, Cell 7: Final Evaluation & Report ===\n",
    "\n",
    "print(\"--- Running Final Evaluation on Test Set ---\")\n",
    "\n",
    "# Run the evaluation function one last time\n",
    "final_loss, final_acc, final_f1, final_auc = evaluate_epoch(\n",
    "    model, test_loader, criterion\n",
    ")\n",
    "\n",
    "print(\"\\n--- Deep Learning Model Metrics (Test Set) ---\")\n",
    "print(f\"     Accuracy: {final_acc*100:.2f}%\")\n",
    "print(f\"     F1-Score: {final_f1:.4f}\")\n",
    "print(f\"ROC AUC Score: {final_auc:.4f}\")\n",
    "\n",
    "# --- Generate a detailed Classification Report ---\n",
    "# We need to get the full list of predictions and targets\n",
    "\n",
    "model.eval()\n",
    "all_targets = []\n",
    "all_preds_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for features, targets in test_loader:\n",
    "        predictions = model(features)\n",
    "        all_preds_probs.extend(predictions.numpy())\n",
    "        all_targets.extend(targets.numpy())\n",
    "\n",
    "# Convert probabilities to binary 0/1 predictions\n",
    "all_preds_binary = np.round(all_preds_probs)\n",
    "\n",
    "print(\"\\n--- Classification Report ---\")\n",
    "# This report shows precision, recall, and f1-score for both classes (0 and 1)\n",
    "report = classification_report(all_targets, all_preds_binary, target_names=['Class 0 (Paid)', 'Class 1 (Default)'])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae011dbd-919a-4d51-8a55-65bd06f6501b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (shodh_env)",
   "language": "python",
   "name": "shodh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
